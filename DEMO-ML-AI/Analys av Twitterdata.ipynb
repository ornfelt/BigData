{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analys av tweets fr√•n bokm√§ssan\n",
    "\n",
    "## Attribution David Johnsson, Uppsala University\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starta med att ladda in f√∂ljande moduler och s√§tt upp visualiseringsmilj√∂n f√∂r matplotlib\n",
    "\n",
    "1. `pandas` \n",
    "2. `textmining` \n",
    "Funktioner f√∂r statistisk textmining, fokuserad p√• bag-of-words model (som ni inte beh√∂ver s√§tta er in f√∂r denna kurs.f F√∂r den nyfikne eller vetgirige finns enkla f√∂rklaringar exempelvis [h√§r](https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/) eller [h√§r](https://www.geeksforgeeks.org/bag-of-words-bow-model-in-nlp/), en enkel tutorial finns ocks√• [h√§r](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)) \n",
    "3. `wordcloud` - En visualiseringsmodul f√∂r att skapa ordmoln, vilket vi g√∂r i denna laboration.\n",
    "4. `matplotlib` \n",
    "5. `sklearn` -  Scikit-learn,ett pythonbibliotek f√∂r maskininl√§rningsalgoritmer, den kommer vi anv√§nda mycket i b√•de laboration 3 och 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K√∂r denna cell f√∂r att ladda in biblioteken och s√§tta upp v√•r milj√∂\n",
    "import itertools\n",
    "\n",
    "import matplotlib\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import textmining as tm\n",
    "import wordcloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# S√§tt upp visualiseringen\n",
    "%matplotlib inline\n",
    "matplotlib.pyplot.rcParams[\"figure.figsize\"] = [10, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Analys av Twitterdata fr√•n bokm√§ssan\n",
    "\n",
    "Ni har blivit inhyrda som konsulter f√∂r en bokpublicist som vill att du ska ta reda p√• vilka teman och b√∂cker som har f√•tt mest uppm√§rksamhet p√• bokm√§ssan i G√∂teborg 2016. \n",
    "\n",
    "Er uppgift √§r att via Twitterdata unders√∂ka vilka √§mnen som f√•tt speciellt mycket uppm√§rksamhet f√∂r och under bokm√§ssan och presentera ett f√∂rslag till f√∂retaget du arbetar med vad som √§r l√§mpliga debatt√§mnen. \n",
    "\n",
    "Fokus h√§r √§r allts√• p√• att f√∂rst√• data, vilket √§r en viktigt del av pre-processering inf√∂r mer avacerad dataanalys. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Data processing\n",
    "\n",
    "Som alltid beh√∂ver v√•rt data st√§das, i detta fall √§r fokus att sortera bort data som antingen inte g√•r att analysera eller inte √§r intressant fr√•n den r√•textdata vi f√•tt fr√•n Twitter. Den data som givits samlades in fr√•n Twitter fr√•n maj till september 2016.\n",
    "\n",
    "Er datafil finns i repositoriet p√• github  och heter `twitter_book_fair_data.tsv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Ladda data\n",
    "\n",
    "En `.tsv` fil betyder att det √§r en tab-separerad fil med tabelldata (j√§mf√∂rt med ; separerad som vi anv√§nt tidigare)\n",
    "\n",
    "**F2** Starta arbetet med att l√§sa in filen med read_csv() med f√∂ljande parametrar:  encoding=\"utf-8\", sep=\"\\t\" och spara i en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_book_df = pd.read_csv(\n",
    "    \"Data/twitter_book_fair_data.tsv\", encoding=\"utf-8\", sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_book_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_book_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_book_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_book_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "En kolumn √§r speciellt intressant f√∂r v√•r **textanalys**, extrahera den fr√•n den dataframe vi lagrat all data i och skapa en variabel d√§r du placerar denna data, d√∂p variablen till `tweets_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_corpus = twitter_data_book_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Emojis\n",
    "\n",
    "P√• Twitter √§r det v√§ldigt vanligt med emojis üëç ‚ú® üê´ üéâ üöÄ ü§ò.\n",
    "\n",
    "Dessa kan inneh√•lla mycket information som kan vara relevant f√∂r v√•r analys. Dock √§r det ofta sv√•rt att analysera emojis med hj√§lp av vanliga verktug f√∂r NLP(Natural Language Processig). \n",
    "\n",
    "Vi beh√∂ver d√§rf√∂r ta bort dessa ur v√•rt utvalda dataset som skapades i uppgiften ovan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encode2ascii = lambda x: x.encode(\"ascii\", errors=\"ignore\").decode(\"utf-8\")\n",
    "clean_tweets = tweets_corpus.apply(encode2ascii)\n",
    "clean_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Ta bort URLs\n",
    "Det √§r ocks√• vanligt att man p√• Twitter l√§nkar till olika webbplatser med hj√§lp av URL:er, n√§r man g√∂r textanalys p√• twitterdata √§r det vanligt att delar av dessa URL:er dyker upp som \"mest frekventa ord\" vilket p√•verkar v√•r analys negativs. Dessa beh√∂ver d√§rf√∂r ocks√• tas bort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = clean_tweets.str.replace(r\"http\\S+\", \"\")\n",
    "clean_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Funktion f√∂r att hitta mest frekventa ord \n",
    "\n",
    "Ett s√§tt att f√∂rst√• hur olika metoder f√∂r pre-processing p√•verkar ett dataset kan man r√§kna de mest f√∂rekommande orden efter varje operation som utf√∂rs. Eftersom vi kommer vilja utf√∂ra denna r√§kning m√•nga g√•nger under arbetet √§r de l√§mpligt att skapa en funktion f√∂r det som vi kan anropa flera g√•nger.\n",
    "\n",
    "#### Vad √§r en Term Document Matrix (TDM)?\n",
    "\n",
    "En TDM √§r en tabell d√§r antalet unika ord r√§knas f√∂r varje dokument. F√∂r att g√∂ra detta p√• v√•rt Twitterdata √§r det l√§mpligt att skapa en TDM d√§r varje tweet √§r en egen vektor d√§r varje element best√•r av de ord som finns i den tweeten. En tweet med tre unika ord blir allts√• en vektor med tre element. \n",
    "\n",
    "Nedanst√•ende kod skapar denna TDM i form av en funktion med namn `create_term_document_matrix()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Skapar en funktion som tar in argumenten corpus och min_df, om inget v√§rde \n",
    "ges till min_df s√• blir det 1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_term_document_matrix(corpus, min_df=1):\n",
    "    cvec = CountVectorizer(min_df=min_df, stop_words=tm.stopwords)\n",
    "    \"\"\"\n",
    "    CountVectorizer skapar en matris med varje unikt ord i en datasamling\n",
    "    och visar hur ofta de f√∂rekommer. Argumentet stop_words ger anv√§ndaren\n",
    "    m√∂jligheten att v√§lja vilka ord som ska vara stopwords. I detta fall\n",
    "    anv√§nds textminings f√∂rdefinierade stop words. min_df definierar den undre\n",
    "    gr√§nsen f√∂r hur ofta ett ord beh√∂ver f√∂rekomma f√∂r att vara med i matrisen\n",
    "    \"\"\"\n",
    "\n",
    "    tfmatrix = cvec.fit_transform(corpus)\n",
    "    # Transformerar en datasamling tll valfri struktur genom sitt agrument\n",
    "    return pd.DataFrame(data=tfmatrix.toarray(), columns=cvec.get_feature_names())\n",
    "    \"\"\"\n",
    "    Funktionen returnerar en dataframe d√§r datam√§ngden kommer fr√•n tfmatrix och \n",
    "    kolumn namnen fr√•n cvec. I det h√§r fallet √§r kolumnnamnen de unika orden. \n",
    "    Funktionen √§r allts√• en implementation av bag-of-words, d√§r varje f√∂rekomst av varje unikt ord (undantaget stopwords) r√§knas.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testa v√•r nya funktion genom att skapa en TDM endast f√∂r de tre f√∂rsta raderna i `clean_tweets` som kan sorteras ut med `.head(3)` funktionen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_term_document_matrix(clean_tweets.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F11.** Hur m√•nga kolumner skapades i TDM:n?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Svaret beror lite p√• vilka parametrar man l√§gger in i funktionen ovan, men man b√∂r kunna plocka fram antalet med exempelvis shape och ocks√• f√∂rhoppningsvis f√∂rst√• att antalet kolumner representerar antalet unika ord vars f√∂rekomst r√§knas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√∂r att hitta de mest frekvent f√∂rekommander orden i v√•r TDM beh√∂ver vi r√§kna ord. Det √§r ocks√• l√§mpligt med en visualisering √∂ver dessa vanligast f√∂rekommande ord. √Ñven detta kommer vi beh√∂va g√∂ra flera g√•nger och d√§rf√∂r √§r det √•terigen l√§mpligt att definiera en funktion `plot_top_words()` som b√•de r√§knar och plottar orden i ett stapeldiagram. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "En funktion som tar in en datasamling, antalet ord\n",
    "som ska finnas i den lista √∂ver top words som returnerar samt antalet ord som ska visas i stapeldiagramet. \n",
    "Fr√•n b√∂rjan ska listan inneh√•lla 50 ord och diagrammet 30. Det finns dock de som √§ndrat detta n√§r de svarar. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def plot_top_words(tweets, num_word_instances, top_words):\n",
    "    tdm_df = create_term_document_matrix(tweets, min_df=2)\n",
    "    # Anv√§nder den tidigare sakpta funktionen f√∂r att ta fram en matris med ord\n",
    "    # som f√∂rekommer minst 2 ggr\n",
    "    word_frequencies = tdm_df[[x for x in tdm_df.columns if len(x) > 1]].sum()\n",
    "    # Anv√§nder en loop f√∂r att se om ordet i kolumnen inneh√•ller fler √§n en\n",
    "    # bokstav och sumerar sedan kolumnen\n",
    "    sorted_words = word_frequencies.sort_values(ascending=False)\n",
    "    # Vi sorterar sedan orden i fallande ordning d.v.s.\n",
    "    # den mest f√∂rekommande f√∂rst\n",
    "    top_sorted_words = sorted_words[:num_word_instances]\n",
    "    # top_sorted_words √§r sedan de num_word_instances mest f√∂rekommande\n",
    "    # orden fr√•n sorted_words\n",
    "    top_sorted_words[:top_words].plot.bar()\n",
    "    # Sedan f√∂rkortas listan ytterligare med top_words\n",
    "    # som d√§refter ritas upp i ett stapeldiagram\n",
    "    return top_sorted_words\n",
    "    # top_sorted_words med num_word_instances antalet ord returneras som en lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kan vi anv√§nda `plot_top_words()` funktionen f√∂r att r√§kna ut de mest f√∂rekommande orden i hela v√•rt corpus, viktigt att ha t√•lamod dock f√∂r det kan ta ett tag. Nedanst√•ende kod utf√∂r ber√§kningen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = plot_top_words(clean_tweets, 50, 30)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Sm√• bokst√§ver\n",
    "\n",
    "N√§sta steg i pre-processingen av v√•rt dataset (v√•rt corpus) √§r att g√∂ra om alla bokst√§ver till sm√•. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tweets_lowered \u001b[38;5;241m=\u001b[39m \u001b[43mclean_tweets\u001b[49m\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_lowered = clean_tweets.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_words_lowered = plot_top_words(tweets_lowered, 50, 30)\n",
    "top_words_lowered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "F√∂r att underl√§tta att j√§mf√∂ra vad v√•ra anstr√§ngningar f√•r f√∂r resultat kan det vara bra att enkelt kunna j√§mf√∂ra olika listor med top_words.\n",
    "\n",
    "Skapa en ny dataframe som har tv√• kolumner, en med de 20 mest frekventa orden fr√•n`top_words` och en med de 20 mest frekventa orden fr√•n `top_word_lowered`. D√∂p kolumnerna till `Top tweeted clean`och  `Top tweeted lowered`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Top tweeted clean\": top_words[0:20].index,\n",
    "        \"Top tweeted lowered\": top_words_lowered[0:20].index,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(top_words[0:20].index) - set(top_words_lowered[0:20].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korta ord\n",
    "\n",
    "Korta ord har ofta inte n√•gon egentlig betydelse, allts√• beh√∂ver vi inte dessa ord. Typiska s√•dana ord kan vara ja, jo eller nej. Vi best√§mmer oss f√∂r att alla ord som √§r kortare √§n 3 bokst√§ver inte innehar n√•gon betydelse i v√•r analys och tar d√§rmed bort dem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_lowered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_low_no_small = tweets_lowered.str.replace(r\"\\b\\w{1,2}\\b\", \"\")#Jag brukar ge godk√§nt om man fyllt i 2 eller 3 h√§r, kommer inte riktigt ih√•g vilket som √§r r√§tt.\n",
    "tweets_low_no_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skapar ny topplista utan korta ord\n",
    "top_words_low_no_small = plot_top_words(tweets_low_no_small, 50, 30)\n",
    "top_words_low_no_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F21.** Efter att korta ord tagits bort, hur m√•nga g√•nger m√•ste ett ord f√∂rekomma i v√•rt corpus f√∂r att hamna i den nya listan enligt ovan? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Betydelsel√∂sa ord\n",
    "\n",
    "Stop words √§r andra ord som inte √§r korta men som √§nd√• inte har betydelse, dessa kan vara lite besv√§rligare att identifiera och ta bort. En m√∂jlighet √§r att helt enkelt skapa en lista med s√•dana ord och sedan anv√§nda den listan f√∂r att filtrera ut orden ur ett corpus. Vi har ju redan tagit bort alla ord med f√§rre bokst√§ver √§n 3, s√• s√•dana beh√∂ver vi inte l√§gga in i listan. \n",
    "\n",
    "Nedan √§r ett exempel p√• en lista med stoppord som √§r betydelsel√∂sa. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = [\n",
    "    \"och\",\n",
    "    \"det\",\n",
    "    \"att\",\n",
    "    \"i\",\n",
    "    \"en\",\n",
    "    \"jag\",\n",
    "    \"hon\",\n",
    "    \"som\",\n",
    "    \"han\",\n",
    "    \"paa\",\n",
    "    \"den\",\n",
    "    \"med\",\n",
    "    \"var\",\n",
    "    \"sig\",\n",
    "    \"foer\",\n",
    "    \"saa\",\n",
    "    \"till\",\n",
    "    \"aer\",\n",
    "    \"men\",\n",
    "    \"ett\",\n",
    "    \"om\",\n",
    "    \"hade\",\n",
    "    \"de\",\n",
    "    \"av\",\n",
    "    \"icke\",\n",
    "    \"mig\",\n",
    "    \"du\",\n",
    "    \"henne\",\n",
    "    \"daa\",\n",
    "    \"sin\",\n",
    "    \"nu\",\n",
    "    \"har\",\n",
    "    \"inte\",\n",
    "    \"hans\",\n",
    "    \"honom\",\n",
    "    \"skulle\",\n",
    "    \"hennes\",\n",
    "    \"daer\",\n",
    "    \"min\",\n",
    "    \"man\",\n",
    "    \"ej\",\n",
    "    \"vid\",\n",
    "    \"kunde\",\n",
    "    \"naagot\",\n",
    "    \"fraan\",\n",
    "    \"ut\",\n",
    "    \"naer\",\n",
    "    \"efter\",\n",
    "    \"upp\",\n",
    "    \"vi\",\n",
    "    \"dem\",\n",
    "    \"vara\",\n",
    "    \"vad\",\n",
    "    \"oever\",\n",
    "    \"aen\",\n",
    "    \"dig\",\n",
    "    \"kan\",\n",
    "    \"sina\",\n",
    "    \"haer\",\n",
    "    \"ha\",\n",
    "    \"mot\",\n",
    "    \"alla\",\n",
    "    \"under\",\n",
    "    \"naagon\",\n",
    "    \"eller\",\n",
    "    \"allt\",\n",
    "    \"mycket\",\n",
    "    \"sedan\",\n",
    "    \"ju\",\n",
    "    \"denna\",\n",
    "    \"sjaelv\",\n",
    "    \"detta\",\n",
    "    \"aat\",\n",
    "    \"utan\",\n",
    "    \"varit\",\n",
    "    \"hur\",\n",
    "    \"ingen\",\n",
    "    \"mitt\",\n",
    "    \"ni\",\n",
    "    \"bli\",\n",
    "    \"blev\",\n",
    "    \"oss\",\n",
    "    \"din\",\n",
    "    \"dessa\",\n",
    "    \"naagra\",\n",
    "    \"deras\",\n",
    "    \"blir\",\n",
    "    \"mina\",\n",
    "    \"samma\",\n",
    "    \"vilken\",\n",
    "    \"er\",\n",
    "    \"saadan\",\n",
    "    \"vaar\",\n",
    "    \"blivit\",\n",
    "    \"dess\",\n",
    "    \"inom\",\n",
    "    \"mellan\",\n",
    "    \"saadant\",\n",
    "    \"varfoer\",\n",
    "    \"varje\",\n",
    "    \"vilka\",\n",
    "    \"ditt\",\n",
    "    \"vem\",\n",
    "    \"vilket\",\n",
    "    \"sitta\",\n",
    "    \"saadana\",\n",
    "    \"vart\",\n",
    "    \"dina\",\n",
    "    \"vars\",\n",
    "    \"vaart\",\n",
    "    \"vaara\",\n",
    "    \"ert\",\n",
    "    \"era\",\n",
    "    \"vilka\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "N√§r vi skapat v√•r lista √§r det dags att skapa en funktion som tar bort dessa fr√•n ett dokument. Denna funktion √§r kodad i cellen nedan. (Igen strunta i lambda f√∂r tillf√§llet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords = lambda x: \" \".join(y for y in x.split() if y not in my_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen ovan tar allts√• bort stoppord fr√•n ett dokument (allts√• en tweet), f√∂r att ta bort stoppord fr√•n hela v√•rt corpus kan funktionen `.apply()`anv√§ndas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_low_no_small_stopwords = tweets_low_no_small.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_low_no_small_stopwords = plot_top_words(tweets_low_no_small_stopwords, 50, 30)\n",
    "top_words_low_no_small_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Vad √§r skillnaderna mellan de frekvent f√∂rekommande orden i j√§mf√∂relse med v√•ra tidigare listor? Skriv den kod som j√§mf√∂r dessa tre listor `top_words_lowered`, `top_words_low_no_small` and `top_words_low_no_small_stopwords`, titta p√• de f√∂rsta 20 orden i listorna.\n",
    "\n",
    "En variant √§r att anv√§nda koden nedan som j√§mf√∂r listorna och returnerar skillanderna. En annan √§r att √•teranv√§nda koden som skapar en dataframe med de tre listorna. B√•da varianterna finns med nedan. Man kan s√§kert g√∂ra p√• andra s√§tt ocks√•. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(top_words_lowered[0:20].index) - set(top_words_low_no_small_stopwords[0:20].index) - set(top_words_low_no_small[0:20].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Top tweeted clean\": top_words[0:20].index,\n",
    "        \"Top tweeted lowered\": top_words_lowered[0:20].index,\n",
    "        \"Top tweeted no small\": top_words_low_no_small[0:20].index,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Visualisering och rekommendation\n",
    "\n",
    "Dags att visualisera v√•rt resultat och √∂vertyga v√•r klient om att vi hittat de b√§sta debatt√§mnena f√∂r dem! H√§r g√∂r vi det genom att skapa ett word cloud d√§r de mest frekventa orden syns b√§st. \n",
    "\n",
    "Nedanst√•ende kod skapar ett ordmoln f√∂r `top_words_low_no_small_stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40)\n",
    "wordcloud.fit_words(top_words_low_no_small_stopwords.to_dict())\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "N√§r du tittar p√• ordmolnet, √§r det fler ord som borde vara stoppord? Ange n√•gra stycken och f√∂rklara varf√∂r de b√∂r tas bort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Vilket tema rekommenderar ni att publicisten ska ha som debatt√§mne? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "sv",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
